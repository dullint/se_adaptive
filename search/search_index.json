{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CAI Documentation","text":"<p>Welcome to the CAI (Critique and Improve) documentation. CAI is a tool designed to help create and refine critique+rewrite examples for Constitutional AI training, specifically focusing on the critique+rewrite step described in the Constitutional AI paper.</p>"},{"location":"#what-is-constitutional-ai","title":"What is Constitutional AI?","text":"<p>Constitutional AI uses reinforcement learning from AI feedback to tune models to follow a set of human-written principles--collectively referred to as the constitution. In the critique+rewrite step, a teacher model critiques and rewrites an initial completion (generated by the student model) to better adhere to a given principle from the constitution.</p>"},{"location":"#current-principle","title":"Current Principle","text":"<p>The current implementation focuses on the following principle:</p> <p>ADAPTIVE Principle</p> <p>The first letter of each sentence in the assistant's response should spell \"ADAPTIVE\".</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udcdd Manual Drafting: Interactively create and refine critique+rewrite examples with real-time feedback</li> <li>\ud83e\udd16 Auto Generate: Automatically generate new examples based on failure analysis of your test set</li> <li>\ud83d\udcca Visualization: View and analyze your examples collection with adherence validation</li> <li>\ud83d\udcda Versioning: Track and manage different versions of your examples library</li> <li>\ud83d\udcc8 Evaluation: Measure how well your examples help the teacher model follow the principle</li> </ul>"},{"location":"#tool-benefits","title":"Tool Benefits","text":"<ul> <li>Interactive Interface: User-friendly Streamlit interface for creating and managing examples</li> <li>LLM Integration: Direct integration with GPT-4 for assistance in drafting examples</li> <li>Quality Control: Automatic validation of the ADAPTIVE principle adherence</li> <li>Iterative Improvement: Tools for continuous refinement of your examples collection</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with CAI:</p> <ol> <li>Follow the Installation Guide</li> <li>Create your first examples using the Manual Drafting Guide</li> <li>Test your examples with the Evaluation Guide</li> <li>Use Auto Generate to improve your collection</li> </ol>"},{"location":"features/auto-generate/","title":"\ud83e\udd16 Auto Generate","text":"<p>The auto-generate feature helps you automatically create new examples based on failure analysis of existing ones. This page explains how to use this feature effectively.</p>"},{"location":"features/auto-generate/#overview","title":"Overview","text":"<p>The auto-generate process consists of three main steps:</p> <ol> <li>Evaluating current examples</li> <li>Analyzing failures</li> <li>Generating new examples</li> </ol>"},{"location":"features/auto-generate/#how-it-works","title":"How it works","text":""},{"location":"features/auto-generate/#1-evaluating-current-examples","title":"1. Evaluating current examples","text":""},{"location":"features/auto-generate/#evaluation-examples","title":"Evaluation examples","text":"<p>Current examples from the <code>dev</code> version are evaluated on a validation set. This validation set is stored in the <code>data/validation.jsonl</code> file. You can overwrite this file to use your own validation set. The jsonl file should contain a list of objects with the following format:</p> <pre><code>{\n  \"human_prompt\": \"human_prompt\",\n  \"model_answer\": \"model_answer\"\n}\n</code></pre>"},{"location":"features/auto-generate/#adversarial-examples","title":"Adversarial examples","text":"<p>The validation set is built to have adversarial examples. Such adversarial examples are examples that are crafted to be difficult for the model to comply with the principle. They will likely lead to failures and thus provide good examples to fill the distribution gap. Such adversarial examples for the 'ADAPTIVE' principle can be:</p> <p>Adversarial examples</p> <ul> <li>\"Write me a poem where each line begin by the sun raising in the east\" -&gt; prompt that forces to always have the same beginning</li> <li>\"Explain to me how nuclear fusion works, keep it simple, in about 3 sentences\" -&gt; prompt that forces to output less sentences than the target number of letters</li> </ul>"},{"location":"features/auto-generate/#2-analyzing-failures","title":"2. Analyzing failures","text":"<p>Once the evaluation is done, the teacher model will be given with the validation results and will focus on the failures. It will then generate an analysis of the failures, focusing on the following points:</p> <ul> <li>Common patterns in the failures</li> <li>Concrete suggestions for generating better examples that would help the model learn</li> <li>What types of examples would be most helpful to add</li> </ul> <p></p>"},{"location":"features/auto-generate/#3-generating-new-examples","title":"3. Generating new examples","text":"<p>The teacher model will then be given the analysis to generate 3 new human prompts that are similar to the ones in the failures. A model answer is then naturally generated for each new human prompt.</p> <p>The critique and the rewrite are then generated with the same process as the manual drafting process, but augmented with the analysis and the failures in order to generate better examples avoiding the pitfalls that lead to failures.</p> <p>Tool limitation</p> <p>Note that the current tool can only generated 3 examples at a time. further development could allow to have a slider allowing to select the number of examples to generate.</p>"},{"location":"features/auto-generate/#4-adding-new-examples-to-the-library","title":"4. Adding new examples to the library","text":"<p>The new examples can then added to the <code>dev</code> version. Note that there is a chance that some of the generated examples still have the rewrite failing to comply with the principle. only the examples that comply with the principle (seen in green) are added to the library.</p>"},{"location":"features/evaluation/","title":"\ud83d\udcc8 Evaluation","text":"<p>The evaluation feature allows you to measure how well your examples help the model follow the ADAPTIVE principle. This page explains how to use the evaluation system effectively.</p>"},{"location":"features/evaluation/#overview","title":"Overview","text":"<p>The evaluation process:</p> <ol> <li>Tests your examples against a test set</li> <li>Measures success rate</li> <li>Generates reports</li> <li>Saves version snapshots</li> </ol>"},{"location":"features/evaluation/#running-an-evaluation","title":"Running an Evaluation","text":""},{"location":"features/evaluation/#starting-an-evaluation","title":"Starting an Evaluation","text":"<ol> <li>Select the version to evaluate from the sidebar</li> <li>Click \"\ud83d\ude80 Run Evaluation\" to begin</li> <li>Monitor progress in real-time</li> </ol>"},{"location":"features/evaluation/#test-dataset","title":"Test Dataset","text":"<p>The evaluation uses a dedicated test set stored in <code>data/test.jsonl</code>. This set:</p> <ul> <li>Is separate from the validation set used in auto-generate</li> <li>Contains diverse examples given in the initial assignment</li> </ul> <p>Test Set Separation</p> <p>Never use examples from the test set in your development version to maintain accurate evaluation metrics.</p>"},{"location":"features/evaluation/#evaluation-process","title":"Evaluation Process","text":""},{"location":"features/evaluation/#step-by-step-evaluation","title":"Step-by-Step Evaluation","text":"<p>For each test example, the system:</p> <ol> <li>Runs the critique+rewrite pipeline</li> <li>Checks if the rewrite follows the ADAPTIVE principle</li> <li>Records results and displays them in real-time</li> </ol>"},{"location":"features/evaluation/#evaluation-reports","title":"Evaluation Reports","text":"<p>After evaluation completes, a report is automatically generated with:</p> <ul> <li>results for each example</li> <li>success metrics</li> </ul> <p>Example report:</p> <pre><code>  \"version\": \"v0\",\n  \"timestamp\": \"20250209_150823\",\n  \"accuracy\": 0.85,\n  \"results\": [\n    {\n      \"human_prompt\": \"I need assistance to get the damn bills from {{Person Name}}\",\n      \"assistant_answer\": \"Certainly! I completely understand ...\",\n      \"critique\": \"The assistant's response does not comply ...\",\n      \"rewrite\": \"Absolutely, I understand the urgency to...\",\n      \"follows_principle\": true,\n      \"first_letters\": \"ADAPTIVE\"  \n    },\n    ...\n  ]\n</code></pre>"},{"location":"features/evaluation/#version-management","title":"Version Management","text":""},{"location":"features/evaluation/#automatic-version-creation","title":"Automatic Version Creation","text":"<p>When evaluating the dev version, a new version is automatically created and results are associated with the new version. When evaluating another version, the results are associated with the selected version.</p>"},{"location":"features/evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Testing: Run evaluations frequently during development</li> <li>Version Control: Create new versions for significant changes</li> <li>Test Set Maintenance: Keep the test set diverse and challenging</li> <li>Metric Tracking: Monitor success rate trends across versions</li> <li>Failure Analysis: Use failed cases to guide improvements</li> </ol>"},{"location":"features/export-import-examples/","title":"Export &amp; Import Examples","text":"<p>CAI stores examples in JSONL files that can be easily exported and imported. This guide explains how to manage your examples.</p>"},{"location":"features/export-import-examples/#file-location","title":"File Location","text":"<p>Examples are stored in the <code>examples</code> directory of your CAI installation:</p> <pre><code>src/cai/examples/\n\u251c\u2500\u2500 ex_dev.jsonl      # Development version examples\n\u251c\u2500\u2500 ex_v1.jsonl       # Version 1 examples\n\u2514\u2500\u2500 ex_v2.jsonl       # Version 2 examples\n</code></pre>"},{"location":"features/export-import-examples/#file-format","title":"File Format","text":"<p>Examples are stored in JSONL format (JSON Lines), where each line is a complete example:</p> <pre><code>{\n  \"human_prompt\": \"Write a greeting\",\n  \"assistant_answer\": \"Initial response\",\n  \"critique\": \"Critique of the response\",\n  \"rewrite\": \"Rewritten response\"\n}\n</code></pre>"},{"location":"features/export-import-examples/#exporting-examples","title":"Exporting Examples","text":""},{"location":"features/export-import-examples/#manual-export","title":"Manual Export","text":"<ol> <li>Navigate to the examples directory</li> <li>Copy the desired <code>.jsonl</code> file</li> <li>Store it in your backup location</li> </ol>"},{"location":"features/export-import-examples/#programmatic-export","title":"Programmatic Export","text":"<p>You can also export examples programmatically:</p> <pre><code>from cai.versioning import load_examples\n\n# Load examples from a specific version\nexamples = load_examples(\"dev\")\n\n# Write to a new file\nwith open(\"my_examples.jsonl\", \"w\") as f:\n    for example in examples:\n        f.write(json.dumps(example) + \"\\n\")\n</code></pre>"},{"location":"features/export-import-examples/#importing-examples","title":"Importing Examples","text":""},{"location":"features/export-import-examples/#manual-import","title":"Manual Import","text":"<ol> <li>Place your <code>.jsonl</code> file in the examples directory</li> <li>Rename it to match the version format (e.g., <code>ex_v3.jsonl</code>)</li> <li>Press R to reload the app</li> </ol> <p>File Format</p> <p>When importing examples, ensure:</p> <ul> <li>The file is in valid JSONL format</li> <li>Each example has all required fields.</li> </ul> <p>Format check</p> <p>Note that the app has Pydantic validation to ensure that the file is valid.</p>"},{"location":"features/manual-drafting/","title":"\ud83d\udcdd Manual Drafting","text":"<p>The manual drafting interface allows you to interactively create and refine examples for Constitutional AI training. This page explains how to use the interface effectively.</p>"},{"location":"features/manual-drafting/#overview","title":"Overview","text":"<p>The manual drafting tool provides a step-by-step workflow for:</p> <ol> <li>Creating initial prompts and responses</li> <li>Critiquing and rewriting responses</li> <li>Refining critiques and rewrites</li> <li>Building an iteration history</li> <li>Adding few-shot examples to your library</li> </ol>"},{"location":"features/manual-drafting/#basic-workflow","title":"Basic Workflow","text":""},{"location":"features/manual-drafting/#1-creating-initial-input","title":"1. Creating Initial Input","text":"<p>Start by entering:</p> <ul> <li>A human prompt in the prompt field</li> <li>An initial model answer</li> </ul> <p>You can either:</p> <ul> <li>Type your own model answer</li> <li>Click \"\ud83e\udd16 Generate Answer\" to have the LLM generate a response and modify it if needed</li> </ul> <p></p>"},{"location":"features/manual-drafting/#2-critiquing-and-rewriting","title":"2. Critiquing and Rewriting","text":"<p>After entering the initial input, you can pre-generate a critique and rewrite.</p> <p>Info</p> <p>We generate the critique and rewrite in one go because we observed that it was necessary to have the rewrite to better improve the critique. However the generation is sequential (following Constitutional AI paper from Anthropic):</p> <ul> <li>The critique is generated based only on the human prompt and the model answer.</li> <li>The rewrite is generated based on the human prompt, the model answer and the critique.</li> </ul> <p></p>"},{"location":"features/manual-drafting/#3-refining-the-critique-and-rewrite","title":"3. Refining the Critique and Rewrite","text":"<p>We saw during our usage of the tool that the critique was the most important part of the crafting process and that it required several iterations to get it good. Once the critique is pre-generated, you can modify it directly by modifying the critique text field or by clicking the \"Refine critique\" button.</p> <p>The refine critique button will let you give natural language instructions to an LLM to improve the critique. Once the critique is refined, you can refine the rewrite by clicking the \"\ud83d\udd04 Regenerate Rewrite\" button.</p> <p></p>"},{"location":"features/manual-drafting/#4-building-an-iteration-history","title":"4. Building an iteration history","text":"<p>As stated in the Constitutional AI paper:</p> <p>Constitutional AI</p> <p>Note that since the final prompt-revision pair is formatted in the same manner as the original prompt-response pair, we can apply the same critique-revision pipeline multiple times, giving us a sequence of revisions.</p> <p>With this in mind, the tool has been designed to allow you to iterate over the results of the critique+rewrite step. You can add an iteration by clicking on the \"\ud83d\udd04 Run New Iteration\" button. this will keep the same human prompt and inject the last rewrite in the model answer field. Every previous iteration is kept visible on the page.</p>"},{"location":"features/manual-drafting/#5-adding-examples-to-your-library","title":"5. Adding examples to your library","text":"<p>Once you are happy with the example, you can add it to your few-shot examples library by clicking on the \"\ud83d\udcda Add to Examples\" button. Note that the tool has a versioning system (More info here). The current version is called the <code>dev</code>version and new examples are always added to this version.</p>"},{"location":"features/visualization-versioning/","title":"\ud83d\udcca Visualization &amp; Versioning","text":"<p>The visualization and versioning interface allows you to manage, view, and analyze your examples across different versions. This page explains how to effectively use these features.</p> <p></p>"},{"location":"features/visualization-versioning/#version-management","title":"Version Management","text":""},{"location":"features/visualization-versioning/#selecting-versions","title":"Selecting Versions","text":"<p>At the top of the visualization page, you'll find version controls:</p>"},{"location":"features/visualization-versioning/#version-types","title":"Version Types","text":"<ul> <li><code>dev</code>: Development version for active work</li> <li><code>v1</code>, <code>v2</code>: past versions or snapshots</li> </ul>"},{"location":"features/visualization-versioning/#actions-on-dev-version","title":"Actions on dev version","text":"<p>The dev version is the version that is currently being worked on.</p> <ul> <li> <p>If the dev version is selected, you can see a \"Save current version\" button. that will create a new version with the current examples. This new version will automatically get the next available number.</p> </li> <li> <p>If an older saved version is selected, you can see a \"Reload to dev\" button. that will reload the dev version with the examples of the selected version.</p> <p>Warning</p> <p>Be careful, this action will overwrite the current dev version with the examples of the selected version.</p> </li> </ul> <p></p>"},{"location":"features/visualization-versioning/#example-visualization","title":"Example Visualization","text":"<p>Each example in the selected version is displayed with:</p> <ul> <li>\ud83d\udcdd Human Prompt</li> <li>\ud83e\udd16 Model Answer</li> <li>\ud83d\udd0d Critique</li> <li>\u2728 Rewrite</li> <li>\u2705/\u274c Adherence Status</li> </ul> <p>The adherence status will allow you to very quickly see if all few-shot examples are compliant with the ADAPTIVE principle.</p>"},{"location":"features/visualization-versioning/#managing-examples","title":"Managing Examples","text":""},{"location":"features/visualization-versioning/#deleting-examples","title":"Deleting Examples","text":"<p>Each example has a delete button that:</p> <ol> <li>Removes the example from the current version</li> <li>Updates the display immediately</li> <li>Maintains version history</li> </ol>"},{"location":"features/visualization-versioning/#adding-new-examples","title":"Adding New Examples","text":"<p>Examples can be added to versions through:</p> <ul> <li>Manual drafting interface</li> <li>Auto-generate feature</li> <li>Direct file upload</li> </ul>"},{"location":"features/visualization-versioning/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Auto Generate features</li> <li>Understand Evaluation metrics</li> <li>Explore Manual Drafting techniques</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing CAI, ensure you have the following prerequisites:</p> <ul> <li>Python 3.11 or higher</li> <li>pip (Python package installer)</li> <li>OpenAI API key</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-development-installation","title":"1. Development Installation","text":"<p>The app is currently not available on PyPI, so you will need to install it from source.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone git@github.com:dullint/se_adaptive.git\n</code></pre> <p>This will install CAI in the current directory and make it editable.</p> <ol> <li>Install the dependencies:</li> </ol> <pre><code>pip install -e .\n</code></pre> <p>To install the dependencies for testing and documentation too, run:</p> <pre><code>pip install -e .[test,docs]\n</code></pre> <p>If environment issues arise, we recommend to install <code>poetry</code> and use it to install the dependencies.</p> <pre><code>pip install poetry\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#2-environment-setup","title":"2. Environment Setup","text":"<p>CAI requires an OpenAI API key to function. Two options are available:</p> <ul> <li>Export the API key as an environment variable:</li> </ul> <pre><code>export OPENAI_API_KEY=&lt;your_openai_api_key&gt;\n</code></pre> <ul> <li>Create a <code>.env</code> file in your project root directory and add your API key:</li> </ul> <pre><code>cp .env.example .env\n</code></pre> .env<pre><code>OPENAI_API_KEY=&lt;your_openai_api_key&gt;\n</code></pre> <p>!!! warning \"API Key Security\"   Never commit your <code>.env</code> file to version control.</p>"},{"location":"getting-started/installation/#running-the-app","title":"Running the App","text":"<p>To run the app, use the following command:</p> <pre><code>cai-app\n</code></pre> <p>or with poetry:</p> <pre><code>poetry run cai-app\n</code></pre> <p>This will start the app in your default browser.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with CAI in just a few minutes.</p>"},{"location":"getting-started/quickstart/#launch-the-application","title":"Launch the Application","text":"<p>After installing CAI, launch the application by running:</p> <pre><code>cai-app\n</code></pre> <p>This will:</p> <ol> <li>Start the CAI application</li> <li>Open your default web browser</li> <li>Display the CAI interface</li> </ol> <p>Tip</p> <p>If your browser doesn't open automatically, look for a URL in your terminal (usually <code>http://localhost:8501</code>)</p> <p>API Key Required</p> <p>Ensure you've configured your OpenAI API key in the <code>.env</code> file as described in the installation guide</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":"<p>When using the app for the first time, you can have a look at the Manual Drafting page to get familiar with the interface.</p> <p></p> <p>Tip</p> <p>The app has a version system. The current prompt version is called the <code>dev</code> version. It is the version you are currently working on. More information about the version system can be found in the versioning guide.</p>"},{"location":"getting-started/quickstart/#1-creating-an-example","title":"1. Creating an Example","text":"<ol> <li>Enter a human prompt</li> <li>Either: type a model answer, or click \"\ud83e\udd16 Generate Answer\" to get an AI response</li> <li>Click \"\u2728 Critique &amp; Rewrite\" to generate improvements</li> </ol>"},{"location":"getting-started/quickstart/#2-checking-adherence","title":"2. Checking Adherence","text":"<p>The interface automatically validates if responses follow the ADAPTIVE principle:</p> <ul> <li>\u2705 Green background: Response follows the principle</li> <li>\u274c Red background: Rewrite is not valid, the example needs improvement</li> </ul>"},{"location":"getting-started/quickstart/#3-saving-examples","title":"3. Saving Examples","text":"<p>When satisfied with an example:</p> <ol> <li>Click \"\ud83d\udcda Add to Examples\" to save it (it will be saved in the <code>dev</code> version)</li> <li>View your saved examples in the Visualization page</li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Manual Drafting features</li> <li>Explore Auto Generate capabilities</li> <li>Use Evaluation to test your examples</li> </ul>"}]}